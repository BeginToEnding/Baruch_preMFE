\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{graphicx} % Required for \scalebox
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage[extreme]{savetrees} % tighter margins
\usepackage{lmodern} % Use scalable Latin Modern fonts

\usepackage{bm}
\let\normalmu\mu
\let\normalSigma\Sigma
\renewcommand{\mu}{\bm{\normalmu}} % I want bold mu
\renewcommand{\Sigma}{\bm{\normalSigma}} % I want bold Sigma


\title{Baruch NLA SIP HW 1}
\author{group 1}

\begin{document}
\maketitle
\tableofcontents

\section{exercise 1 Daniel}
Let \(A = (a_{ij}) \in \mathbb{R}^{n\times n}\) and let \(D\) be diagonal with entries \(d_1, \dots, d_n\).
We use the Einstein summation convention (repeated indices are summed).

\[
    (AD)_{ij} = a_{ik} D_{kj} = a_{ik} \, d_k \, \delta_{kj} = a_{ij} \, d_j,
\]
\[
    (DA)_{ij} = D_{ik} a_{kj} = d_i \, \delta_{ik} \, a_{kj} = d_i \, a_{ij}.
\]

Thus
\[
    AD = DA \quad \Longleftrightarrow \quad (d_j - d_i)\, a_{ij} = 0
\]
for all \(i,j\) and all diagonal \(D\).

If \(A\) is diagonal, then \(a_{ij} = 0\) for \(i \neq j\), so \((d_j - d_i)a_{ij} = 0\) holds and \(AD = DA\).

Conversely, suppose \(AD = DA\) for every diagonal \(D\).
Fix \(i \neq j\) and choose \(D\) with \(d_i \neq d_j\) (e.g., \(d_i = 1,\ d_j = 0\), others arbitrary).
Then
\[
    (d_j - d_i)\, a_{ij} = 0 \quad \Longrightarrow \quad a_{ij} = 0.
\]
Since this holds for all \(i \neq j\), \(A\) is diagonal.

Therefore, \(AD = DA\) for any diagonal \(D\) if and only if \(A\) is diagonal.

\section{exercise 2 Daniel}
$M_2 M_3 M_4=M_1^{-1}$ because if we multiply both sides by $M_1$ from the left, we get $M_1 M_2 M_3 M_4 = I$, and the inverse matrix is unique.

Multiply the original statement by $M_1$ from the right and by $M_1^{-1}$ from the left to get $M_2 M_3 M_4 M_1 = I$. So we showed the first statement.
With this new truth, we can derive the inverse for $M_2$ with the same logic and get the 2nd statement,
from which we can derive the inverse for $M_3$, and confirm the 3rd statement, similarly for $M_4$ and for the last statement.

\section{exercise 3 Daniel}
Left-multiply by $I+A$ and right-multiply by $(I+A^{-1})$, which exists, as we can see from the problem statement:

\begin{align*}
    (I+A)(I+A)^{-1}(I+A^{-1})
     & = (I+A)\Bigl[I-(I+A^{-1})^{-1}\Bigr](I+A^{-1}) \\[2mm]
    I\,(I+A^{-1})
     & = (I+A)\Bigl[(I+A^{-1})-I\Bigr]                \\[2mm]
    I+A^{-1}
     & = (I+A)A^{-1}                                  \\
     & = I+A^{-1}.
\end{align*}
We made equivalent transformations throughout the process, the last equation is true, so the first equation must be true too.
\section{exercise 4}
\section{exercise 5}
\section{exercise 6 Daniel}
Wow! Multiply by $A+B$ from the left:
\[\begin{gathered}
        I + B{A^{ - 1}} + A{B^{ - 1}} + I = I\quad X: = B{A^{ - 1}} \hfill \\
        X + {X^{ - 1}} =  - I\quad /X \cdot  \hfill \\
        {X^2} + X + I = 0 \hfill \\
        \left( {{X^2} + X + I} \right)\left( {X - I} \right) = 0\left( {X - I} \right) \qquad \text{thanks, chatGPT}\hfill \\
        {X^3} - I = 0 \hfill \\
        {X^3} = I \Rightarrow \det \left( {{X^3}} \right) = 1 = {\left( {\frac{{\det \left( B \right)}}{{\det \left( A \right)}}} \right)^3}\quad {\text{if }}\det \left( A \right),\det \left( B \right) \in \mathbb{R} \Rightarrow \det \left( A \right) = \det \left( B \right) \hfill \\
    \end{gathered} \]

\section{exercise 7 Daniel}
% Covariance, variances, and correlation matrix
\[
    \Sigma =
    \begin{pmatrix}
        1      & -0.525 & 1.375  & -0.075 & -0.75  \\
        -0.525 & 4      & 0.1875 & 0.1875 & -0.675 \\
        1.375  & 0.1875 & 12.25  & 0.4375 & -1.875 \\
        -0.075 & 0.1875 & 0.4375 & 6.25   & 0.3    \\
        -0.75  & -0.675 & -1.875 & 0.3    & 4.41
    \end{pmatrix}.
\]

% Diagonal matrix of variances and the std-dev matrix
\[
    D=\mathrm{diag}(\Sigma)=
    \begin{pmatrix}
        1 & 0 & 0     & 0    & 0    \\
        0 & 4 & 0     & 0    & 0    \\
        0 & 0 & 12.25 & 0    & 0    \\
        0 & 0 & 0     & 6.25 & 0    \\
        0 & 0 & 0     & 0    & 4.41
    \end{pmatrix},
    \qquad
    S:=D^{1/2}=\mathrm{diag}(1,\,2,\,3.5,\,2.5,\,2.1).
\]

% Definition of the correlation matrix
\[
    \Omega \;=\; D^{-1/2}\,\Sigma\,D^{-1/2}
    \quad\Longleftrightarrow\quad
    \omega_{ij}=\frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}.
\]

% (Optional) exact fractional form
\[
    \Omega =
    \begin{pmatrix}
        1               & -\tfrac{21}{80} & \tfrac{11}{28}  & -\tfrac{3}{100} & -\tfrac{5}{14}  \\
        -\tfrac{21}{80} & 1               & \tfrac{3}{112}  & \tfrac{3}{80}   & -\tfrac{9}{56}  \\
        \tfrac{11}{28}  & \tfrac{3}{112}  & 1               & \tfrac{1}{20}   & -\tfrac{25}{98} \\
        -\tfrac{3}{100} & \tfrac{3}{80}   & \tfrac{1}{20}   & 1               & \tfrac{2}{35}   \\
        -\tfrac{5}{14}  & -\tfrac{9}{56}  & -\tfrac{25}{98} & \tfrac{2}{35}   & 1
    \end{pmatrix}.
\]

% Numerical result (6 d.p.)
\[
    \Omega \approx
    \begin{pmatrix}
        1         & -0.262500 & 0.392857  & -0.030000 & -0.357143 \\
        -0.262500 & 1         & 0.026786  & 0.037500  & -0.160714 \\
        0.392857  & 0.026786  & 1         & 0.050000  & -0.255102 \\
        -0.030000 & 0.037500  & 0.050000  & 1         & 0.057143  \\
        -0.357143 & -0.160714 & -0.255102 & 0.057143  & 1
    \end{pmatrix}.
\]


\section{exercise 8}
\section{exercise 9}
\section{exercise 10}
\section{exercise 11}
\section{exercise 12}
\section{exercise 13}
\section{exercise 14}
\section{exercise 15 Daniel}
The matrix is only of 4x4 and writing out all the equations would actually lead to a 4th order polynomial that can be reduced to a 2nd order polynomial.

We can also look up the Dan's LA Primer book for this example.

Another way is to get reminded by ChatGPT that we can reduce the problem to a linear, homogeneous, constant-coefficient recurrence relation. This looks like a pretty general approch so I go with this one.

Let the eigenvector be $\mathbf{p} = \begin{pmatrix} p_1 \\ p_2 \\ p_3 \\ p_4 \end{pmatrix}$, then we have the following system of equations:
\[ - {p_{j - 1}} + \left( {2 - \lambda } \right){p_j} - {p_{j + 1}} = 0\qquad \forall j \in \{ 1,2,3,4\} \quad {p_0} = {p_5} = 0\]
We solve it with the ansatz $p_j = r^j$ for some $r$. Plugging in gives us the characteristic equation:
\[ - r^{j-1} + (2 - \lambda) r^j - r^{j+1} = 0 \]
Dividing by $r^{j-1}$ (for $r \neq 0$) leads to:
\[ - 1 + (2 - \lambda) r - r^2 = 0 \]
Rearranging gives the standard form of a quadratic equation:
\[ r^2 - (2 - \lambda) r + 1 = 0 \]
Note that if $r$ is a root, then $r^{-1}$ is also a root, so we can write the general solution as
\[ p_j = A r^j + B r^{-j} \]
for some constants $A$ and $B$. $B=-A$ because $p_0 = 0$, so we can write
\[ p_j = A \left( r^j - r^{-j} \right) \]
At $p_5  =0$ we have
\[ 0 = A \left( r^5 - r^{-5} \right) \]
This implies that either $A = 0$ or $r^5 = r^{-5}$, which leads to $r^{10} = 1$. The solutions to this equation are the 10th roots of unity:
\[ r = e^{i\frac{2\pi k}{10}}, \quad k = 0, 1, \ldots, 4 \]
Note that $k>4$ result in the same solutions due to $B=-A$ and the periodicity of the exponential fucntion. From the quadratic equation, divided by $r$:
\[2-\lambda_k = e^{i\frac{2\pi k}{10}} + e^{-i\frac{2\pi k}{10}} = 2 \cos\left(\frac{2\pi k}{10}\right)\quad \]
So the matrix has the diagonal form of
\[
    \Lambda = \begin{pmatrix}
        2 - 2\cos\left(\frac{\pi}{5}\right) & 0                                    & 0                                    & 0                                    \\
        0                                   & 2 - 2\cos\left(\frac{2\pi}{5}\right) & 0                                    & 0                                    \\
        0                                   & 0                                    & 2 - 2\cos\left(\frac{3\pi}{5}\right) & 0                                    \\
        0                                   & 0                                    & 0                                    & 2 - 2\cos\left(\frac{4\pi}{5}\right)
    \end{pmatrix}
\]

\section{exercise 16}
\section{exercise 17 Daniel}
\[\begin{gathered}
        \Omega  = \rho {\mathbf{1}}{{\mathbf{1}}^T} + \left( {\rho  - 1} \right){\mathbf{I}} \hfill \\
        \Omega v = \left( {\rho {\mathbf{1}}{{\mathbf{1}}^T} + \left( {\rho  - 1} \right){\mathbf{I}}} \right)v \hfill \\
        \Omega v = \rho {\mathbf{1}}\left( {{{\mathbf{1}}^T}v} \right) + \left( {1 - \rho } \right)v \Rightarrow \begin{array}{*{20}{c}}
            {{v_1} = {\mathbf{1}}\qquad {\lambda _1} = \rho n + 1 - \rho  \geqslant 0 \Rightarrow \boxed{\rho  \geqslant \frac{{ - 1}}{{n - 1}}}} \\
            {{\mathbf{1}^T}{v_{2,3, \ldots }} = 0\quad {\lambda _{2,3, \ldots }} = 1 - \rho  \geqslant 0 \Rightarrow \boxed{\rho  \leqslant 1}}
        \end{array} \hfill \\
    \end{gathered} \]
\section{exercise 18 Daniel}
The matrix can be written as
\[A= d{{\mathbf{I}}_{n \times n}} + {\mathbf{1}} \left( {1,2, \ldots ,n} \right)\]
The identity matrix has every vector as an eigenvector, and the rank-1 matrix has only 1 eigenvector $\mathbf{v}$ outside the diad's kernel, for which:
\[
    \hat\lambda \mathbf{v} = \left[{\mathbf{1}} \left( {1,2, \ldots ,n} \right) \right] \cdot \mathbf{v} = {\mathbf{1}} \cdot \left[ \left( {1,2, \ldots ,n} \right) \mathbf{v}\right] \Rightarrow \mathbf{v}_1 = \mathbf{1} \quad \hat\lambda_1 = \frac{n \cdot (n+1)}{2}
\]
And for the kernel:
\[\left[ {{\mathbf{1}}\left( {1,2, \ldots ,n} \right)} \right]{\mathbf{v}} = {\mathbf{1}}\underbrace {\left[ {\left( {1,2, \ldots ,n} \right){\mathbf{v}}} \right]}_0 \Rightarrow \hat\lambda _k = 0\quad \forall k \in \left\{ {2,3, \ldots ,n} \right\}\quad {{\mathbf{v}}_k} = {{\mathbf{e}}_1} - {{\mathbf{e}}_k}/k\]

So the eigenvalues for $A$ are
\begin{itemize}
    \item $\lambda_1 = d + \frac{n \cdot (n+1)}{2}$ and the eigenvector is $\mathbf{v}_1 = \mathbf{1}$.
    \item $\lambda_k = d$ and the eigenvectors are $\mathbf{v}_k = {\mathbf{e}}_1 - {{\mathbf{e}}_k}/k$ for $k \in \left\{2, 3, \ldots n\right\}$
\end{itemize}


\section{exercise 19 Daniel}
Let $v \neq0$, then $Qv \neq 0$ because $Q$ is orthogonal so it keeps the norm.

i) \[{v^t}{Q^t}AQv = {\left( {Qv} \right)^t}A\left( {Qv} \right) > 0\]
must hold because $A$ is spd, and $Qv$ is a non-zero vector. So $Q^tAQ$ is also spd.

ii) \[{v^t}{Q^t}AQv = {\left( {Qv} \right)^t}A\left( {Qv} \right) \geqslant 0\]
must hold because $A$ is spsd. So $Q^tAQ$ is also spsd.

\section{exercise 20 Daniel}
\[A = Q\Lambda {Q^t} \Rightarrow {A^{ - 1}} = {\left( {{Q^t}} \right)^{ - 1}}{\Lambda ^{ - 1}}{Q^{ - 1}}\]
$Q$ orthogonal, $Q^{-1} = Q^t$:
\[\begin{gathered}
        {A^{ - 1}} = Q{\Lambda ^{ - 1}}{Q^t} \hfill \\
        A{A^{ - 1}} = ?I = ?Q\Lambda {Q^t}Q{\Lambda ^{ - 1}}{Q^t} = Q\Lambda {\Lambda ^{ - 1}}{Q^t} = Q{Q^t} = I \hfill \\
    \end{gathered} \]
\section{exercise 21}

\section{exercise 22}
\section{exercise 23 Daniel}
The covariance matrix $\mathbf{\Sigma}$ is symmetric and positive definite,
so we can find an Cholesky decomposition,
using which we can define the random vector $\mathbf{X}$ that has the right properties.

\[{\mathbf{\Sigma }} = {\mathbf{L}}{{\mathbf{L}}^T}\qquad {\mathbf{LZ}} = {\mathbf{X}} \Rightarrow Var\left( {\mathbf{X}} \right) = {\mathbf{L}}{{\mathbf{L}}^T} = {\mathbf{\Sigma }}\]

\[{\mathbf{L}} = \left( {\begin{array}{*{20}{c}}
            a & 0 \\
            d & c
        \end{array}} \right) \Rightarrow \begin{array}{*{20}{c}}
        {{a^2} = 9} \\
        {ab =  - 4} \\
        {{b^2} + {c^2} = 16}
    \end{array} \Rightarrow \begin{array}{*{20}{c}}
        {a = 3}      \\
        {b =  - 4/3} \\
        {c = 8\sqrt 2 /3}
    \end{array} \Rightarrow \begin{array}{*{20}{c}}
        {{X_1} = 3{Z_1}} \\
        {{X_2} =  - 4{Z_1}/3 + 8\sqrt 2 {Z_2}/3}
    \end{array}
\]

\section{exercise 24}
Use notation
\[\left( {\begin{array}{*{20}{c}}
            {{X_1}} \\
            {{X_2}} \\
            {{X_3}}
        \end{array}} \right) = {\mathbf{X}}\sim\mathcal{N}\left( {{\mathbf{\mu }},{\mathbf{\Sigma }}} \right)\]

Let ${\mathbf{\hat X}} = {\mathbf{X}} - {\mathbf{\mu }}$,
then $Var\left( {{\mathbf{\hat X}}} \right) = Var\left( {\mathbf{X}} \right) = {\mathbf{\Sigma }}$.
\[P\left( {{{\mathbf{a}}^T}{\mathbf{X}} > 0} \right) = P\left( {{{\mathbf{a}}^T}\left( {{\mathbf{\mu }} + {\mathbf{\hat X}}} \right) > 0} \right) = P\left( {\underbrace {{{\mathbf{a}}^T}{\mathbf{\mu }} + {{\mathbf{a}}^T}{\mathbf{\hat X}}}_Y} > 0 \right)\]

What is the distribution of $Y$?
\[E\left( {{{\mathbf{a}}^T}{\mathbf{\mu }} + {{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = E\left( {{{\mathbf{a}}^T}{\mathbf{\mu }}} \right) + E\left( {{{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = {{\mathbf{a}}^T}{\mathbf{\mu }} = -4\]
\[Var\left( {{{\mathbf{a}}^T}{\mathbf{\mu }} + {{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = Var\left( {{{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = {{\mathbf{a}}^T}Var\left( {{\mathbf{\hat X}}} \right){\mathbf{a}} = {{\mathbf{a}}^T}{\mathbf{\Sigma a}} = 32\]
We also know that $Y$ is a linear combination of normally distributed variables, so it is also normally distributed:
\[Y\sim \mathcal N({{\mathbf{a}}^T}{\mathbf{\mu }}, {{\mathbf{a}}^T}{\mathbf{\Sigma a}}) = \mathcal N(-4, 32)\]
and
\[Z=\frac{Y - E(Y)}{\sqrt{Var(Y)}} = \frac{Y + 4}{\sqrt{32}} \sim \mathcal N(0, 1)\]
\[P\left( {{{\mathbf{a}}^T}{\mathbf{X}} > 0} \right) = P\left( {Y > 0} \right) = P\left( {\frac{{Y + 4}}{{\sqrt {32} }} > \frac{4}{{\sqrt {32} }}} \right) = P\left( {Z > \frac{4}{{\sqrt {32} }}} \right) \approx 0.24\]
\end{document}
