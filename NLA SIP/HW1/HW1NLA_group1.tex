\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{graphicx} % Required for \scalebox
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage[extreme]{savetrees} % tighter margins
\usepackage{lmodern} % Use scalable Latin Modern fonts

\usepackage{bm}
\let\normalmu\mu
\let\normalSigma\Sigma
\renewcommand{\mu}{\bm{\normalmu}} % I want bold mu
\renewcommand{\Sigma}{\bm{\normalSigma}} % I want bold Sigma


\title{Baruch NLA SIP HW 1}
\author{group 1}

\begin{document}
\maketitle
\tableofcontents

\section{exercise 1 Daniel}
Let \(A = (a_{ij}) \in \mathbb{R}^{n\times n}\) and let \(D\) be diagonal with entries \(d_1, \dots, d_n\).
We use the Einstein summation convention (repeated indices are summed).

\[
    (AD)_{ij} = a_{ik} D_{kj} = a_{ik} \, d_k \, \delta_{kj} = a_{ij} \, d_j,
\]
\[
    (DA)_{ij} = D_{ik} a_{kj} = d_i \, \delta_{ik} \, a_{kj} = d_i \, a_{ij}.
\]

Thus
\[
    AD = DA \quad \Longleftrightarrow \quad (d_j - d_i)\, a_{ij} = 0
\]
for all \(i,j\) and all diagonal \(D\).

If \(A\) is diagonal, then \(a_{ij} = 0\) for \(i \neq j\), so \((d_j - d_i)a_{ij} = 0\) holds and \(AD = DA\).

Conversely, suppose \(AD = DA\) for every diagonal \(D\).
Fix \(i \neq j\) and choose \(D\) with \(d_i \neq d_j\) (e.g., \(d_i = 1,\ d_j = 0\), others arbitrary).
Then
\[
    (d_j - d_i)\, a_{ij} = 0 \quad \Longrightarrow \quad a_{ij} = 0.
\]
Since this holds for all \(i \neq j\), \(A\) is diagonal.

Therefore, \(AD = DA\) for any diagonal \(D\) if and only if \(A\) is diagonal.

\section{exercise 2 Daniel}
$M_2 M_3 M_4=M_1^{-1}$ because if we multiply both sides by $M_1$ from the left, we get $M_1 M_2 M_3 M_4 = I$, and the inverse matrix is unique.

Multiply the original statement by $M_1$ from the right and by $M_1^{-1}$ from the left to get $M_2 M_3 M_4 M_1 = I$. So we showed the first statement.
With this new truth, we can derive the inverse for $M_2$ with the same logic and get the 2nd statement,
from which we can derive the inverse for $M_3$, and confirm the 3rd statement, similarly for $M_4$ and for the last statement.

\section{exercise 3}
\section{exercise 4}
\section{exercise 5}
\section{exercise 6}
\section{exercise 7}
\section{exercise 8}
\section{exercise 9}
\section{exercise 10}
\section{exercise 11}
\section{exercise 12}
\section{exercise 13}
\section{exercise 14}
\section{exercise 15 Daniel}
\section{exercise 16}
\section{exercise 17}
\section{exercise 18 Daniel}
\section{exercise 19}
\section{exercise 20}
\section{exercise 21}
\section{exercise 22}
\section{exercise 23 Daniel}
The covariance matrix $\mathbf{\Sigma}$ is symmetric and positive definite,
so we can find an Cholesky decomposition,
using which we can define the random vector $\mathbf{X}$ that has the right properties.

\[{\mathbf{\Sigma }} = {\mathbf{L}}{{\mathbf{L}}^T}\qquad {\mathbf{LZ}} = {\mathbf{X}} \Rightarrow Var\left( {\mathbf{X}} \right) = {\mathbf{L}}{{\mathbf{L}}^T} = {\mathbf{\Sigma }}\]

\[{\mathbf{L}} = \left( {\begin{array}{*{20}{c}}
            a & 0 \\
            d & c
        \end{array}} \right) \Rightarrow \begin{array}{*{20}{c}}
        {{a^2} = 9} \\
        {ab =  - 4} \\
        {{b^2} + {c^2} = 16}
    \end{array} \Rightarrow \begin{array}{*{20}{c}}
        {a = 3}      \\
        {b =  - 4/3} \\
        {c = 8\sqrt 2 /3}
    \end{array} \Rightarrow \begin{array}{*{20}{c}}
        {{X_1} = 3{Z_1}} \\
        {{X_2} =  - 4{Z_1}/3 + 8\sqrt 2 {Z_2}/3}
    \end{array}
\]

\section{exercise 24}
Use notation
\[\left( {\begin{array}{*{20}{c}}
            {{X_1}} \\
            {{X_2}} \\
            {{X_3}}
        \end{array}} \right) = {\mathbf{X}}\sim\mathcal{N}\left( {{\mathbf{\mu }},{\mathbf{\Sigma }}} \right)\]

Let ${\mathbf{\hat X}} = {\mathbf{X}} - {\mathbf{\mu }}$,
then $Var\left( {{\mathbf{\hat X}}} \right) = Var\left( {\mathbf{X}} \right) = {\mathbf{\Sigma }}$.
\[P\left( {{{\mathbf{a}}^T}{\mathbf{X}} > 0} \right) = P\left( {{{\mathbf{a}}^T}\left( {{\mathbf{\mu }} + {\mathbf{\hat X}}} \right) > 0} \right) = P\left( {\underbrace {{{\mathbf{a}}^T}{\mathbf{\mu }} + {{\mathbf{a}}^T}{\mathbf{\hat X}}}_Y} > 0 \right)\]

What is the distribution of $Y$?
\[E\left( {{{\mathbf{a}}^T}{\mathbf{\mu }} + {{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = E\left( {{{\mathbf{a}}^T}{\mathbf{\mu }}} \right) + E\left( {{{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = {{\mathbf{a}}^T}{\mathbf{\mu }} = -4\]
\[Var\left( {{{\mathbf{a}}^T}{\mathbf{\mu }} + {{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = Var\left( {{{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = {{\mathbf{a}}^T}Var\left( {{\mathbf{\hat X}}} \right){\mathbf{a}} = {{\mathbf{a}}^T}{\mathbf{\Sigma a}} = 32\]
We also know that $Y$ is a linear combination of normally distributed variables, so it is also normally distributed:
\[Y\sim \mathcal N({{\mathbf{a}}^T}{\mathbf{\mu }}, {{\mathbf{a}}^T}{\mathbf{\Sigma a}}) = \mathcal N(-4, 32)\]
and
\[Z=\frac{Y - E(Y)}{\sqrt{Var(Y)}} = \frac{Y + 4}{\sqrt{32}} \sim \mathcal N(0, 1)\]
\[P\left( {{{\mathbf{a}}^T}{\mathbf{X}} > 0} \right) = P\left( {Y > 0} \right) = P\left( {\frac{{Y + 4}}{{\sqrt {32} }} > \frac{4}{{\sqrt {32} }}} \right) = P\left( {Z > \frac{4}{{\sqrt {32} }}} \right) \approx 0.24\]
\end{document}
