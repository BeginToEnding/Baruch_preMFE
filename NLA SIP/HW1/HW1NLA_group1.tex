\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{graphicx} % Required for \scalebox
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage[extreme]{savetrees} % tighter margins
\usepackage{lmodern} % Use scalable Latin Modern fonts

\usepackage{bm}
\let\normalmu\mu
\let\normalSigma\Sigma
\renewcommand{\mu}{\bm{\normalmu}} % I want bold mu
\renewcommand{\Sigma}{\bm{\normalSigma}} % I want bold Sigma


\title{Baruch NLA SIP HW 1}
\author{group 1}

\begin{document}
\maketitle
\tableofcontents

\section{exercise 1 Daniel}
Let \(A = (a_{ij}) \in \mathbb{R}^{n\times n}\) and let \(D\) be diagonal with entries \(d_1, \dots, d_n\).
We use the Einstein summation convention (repeated indices are summed).

\[
    (AD)_{ij} = a_{ik} D_{kj} = a_{ik} \, d_k \, \delta_{kj} = a_{ij} \, d_j,
\]
\[
    (DA)_{ij} = D_{ik} a_{kj} = d_i \, \delta_{ik} \, a_{kj} = d_i \, a_{ij}.
\]

Thus
\[
    AD = DA \quad \Longleftrightarrow \quad (d_j - d_i)\, a_{ij} = 0
\]
for all \(i,j\) and all diagonal \(D\).

If \(A\) is diagonal, then \(a_{ij} = 0\) for \(i \neq j\), so \((d_j - d_i)a_{ij} = 0\) holds and \(AD = DA\).

Conversely, suppose \(AD = DA\) for every diagonal \(D\).
Fix \(i \neq j\) and choose \(D\) with \(d_i \neq d_j\) (e.g., \(d_i = 1,\ d_j = 0\), others arbitrary).
Then
\[
    (d_j - d_i)\, a_{ij} = 0 \quad \Longrightarrow \quad a_{ij} = 0.
\]
Since this holds for all \(i \neq j\), \(A\) is diagonal.

Therefore, \(AD = DA\) for any diagonal \(D\) if and only if \(A\) is diagonal.

\section{exercise 2 Daniel}
$M_2 M_3 M_4=M_1^{-1}$ because if we multiply both sides by $M_1$ from the left, we get $M_1 M_2 M_3 M_4 = I$, and the inverse matrix is unique.

Multiply the original statement by $M_1$ from the right and by $M_1^{-1}$ from the left to get $M_2 M_3 M_4 M_1 = I$. So we showed the first statement.
With this new truth, we can derive the inverse for $M_2$ with the same logic and get the 2nd statement,
from which we can derive the inverse for $M_3$, and confirm the 3rd statement, similarly for $M_4$ and for the last statement.

\section{exercise 3}
\section{exercise 4}
\section{exercise 5}
\section{exercise 6}
\section{exercise 7}
\section{exercise 8}
\section{exercise 9}
\section{exercise 10}
\section{exercise 11}
\section{exercise 12}
\section{exercise 13}
\section{exercise 14}
\section{exercise 15 Daniel}
The matrix is only of 4x4 and writing out all the equations would actually lead to a 4th order polynomial that can be reduced to a 2nd order polynomial.

Another way is to get reminded by ChatGPT that we can reduce the problem to a linear, homogeneous, constant-coefficient recurrence relation.
Let the eigenvector be $\mathbf{p} = \begin{pmatrix} p_1 \\ p_2 \\ p_3 \\ p_4 \end{pmatrix}$, then we have the following system of equations:
\[ - {p_{j - 1}} + \left( {2 - \lambda } \right){p_j} - {p_{j + 1}} = 0\qquad \forall j \in \{ 1,2,3,4\} \quad {p_0} = {p_5} = 0\]
We solve it with the ansatz $p_j = r^j$ for some $r$. Plugging in gives us the characteristic equation:
\[ - r^{j-1} + (2 - \lambda) r^j - r^{j+1} = 0 \]
Dividing by $r^{j-1}$ (for $r \neq 0$) leads to:
\[ - 1 + (2 - \lambda) r - r^2 = 0 \]
Rearranging gives the standard form of a quadratic equation:
\[ r^2 - (2 - \lambda) r + 1 = 0 \]
Note that if $r$ is a root, then $r^{-1}$ is also a root, so we can write the general solution as
\[ p_j = A r^j + B r^{-j} \]
for some constants $A$ and $B$. $B=-A$ because $p_0 = 0$, so we can write
\[ p_j = A \left( r^j - r^{-j} \right) \]
At $p_5  =0$ we have
\[ 0 = A \left( r^5 - r^{-5} \right) \]
This implies that either $A = 0$ or $r^5 = r^{-5}$, which leads to $r^{10} = 1$. The solutions to this equation are the 10th roots of unity:
\[ r = e^{i\frac{2\pi k}{10}}, \quad k = 0, 1, \ldots, 4 \]
Note that $k>4$ result in the same solutions due to $B=-A$ and the periodicity of the exponential fucntion. From the quadratic equation, divided by $r$:
\[2-\lambda_k = e^{i\frac{2\pi k}{10}} + e^{-i\frac{2\pi k}{10}} = 2 \cos\left(\frac{2\pi k}{10}\right)\quad \]
So the matrix has the diagonal form of
\[
    \Lambda = \begin{pmatrix}
        2 - 2\cos\left(\frac{\pi}{5}\right) & 0                                    & 0                                    & 0                                    \\
        0                                   & 2 - 2\cos\left(\frac{2\pi}{5}\right) & 0                                    & 0                                    \\
        0                                   & 0                                    & 2 - 2\cos\left(\frac{3\pi}{5}\right) & 0                                    \\
        0                                   & 0                                    & 0                                    & 2 - 2\cos\left(\frac{4\pi}{5}\right)
    \end{pmatrix}
\]

\section{exercise 16}
\section{exercise 17 Daniel}
\[\begin{gathered}
        \Omega  = \rho {\mathbf{1}}{{\mathbf{1}}^T} + \left( {\rho  - 1} \right){\mathbf{I}} \hfill \\
        \Omega v = \left( {\rho {\mathbf{1}}{{\mathbf{1}}^T} + \left( {\rho  - 1} \right){\mathbf{I}}} \right)v \hfill \\
        \Omega v = \rho {\mathbf{1}}\left( {{{\mathbf{1}}^T}v} \right) + \left( {1 - \rho } \right)v \Rightarrow \begin{array}{*{20}{c}}
            {{v_1} = {\mathbf{1}}\qquad {\lambda _1} = \rho n + 1 - \rho  \geqslant 0 \Rightarrow \boxed{\rho  \geqslant \frac{{ - 1}}{{n - 1}}}} \\
            {{v_{2,3, \ldots }}{\mathbf{1}} = 0\quad {\lambda _{2,3, \ldots }} = 1 - \rho  \geqslant 0 \Rightarrow \boxed{\rho  \leqslant 1}}
        \end{array} \hfill \\
    \end{gathered} \]
\section{exercise 18 Daniel}
The matrix can be written as
\[A= d{{\mathbf{I}}_{n \times n}} + {\mathbf{1}} \left( {1,2, \ldots ,n} \right)\]
The identity matrix has every vector as an eigenvector, and the rank-1 matrix has only 1 eigenvector $\mathbf{v}$ outside the diad's kernel, for which:
\[
    \hat\lambda \mathbf{v} = \left[{\mathbf{1}} \left( {1,2, \ldots ,n} \right) \right] \cdot \mathbf{v} = {\mathbf{1}} \cdot \left[ \left( {1,2, \ldots ,n} \right) \mathbf{v}\right] \Rightarrow \mathbf{v}_1 = \mathbf{1} \quad \hat\lambda_1 = \frac{n \cdot (n+1)}{2}
\]
And for the kernel:
\[\left[ {{\mathbf{1}}\left( {1,2, \ldots ,n} \right)} \right]{\mathbf{v}} = {\mathbf{1}}\underbrace {\left[ {\left( {1,2, \ldots ,n} \right){\mathbf{v}}} \right]}_0 \Rightarrow \hat\lambda _k = 0\quad \forall k \in \left\{ {2,3, \ldots ,n} \right\}\quad {{\mathbf{v}}_k} = {{\mathbf{e}}_1} - {{\mathbf{e}}_k}/k\]

So the eigenvalues for $A$ are
\begin{itemize}
    \item $\lambda_1 = d + \frac{n \cdot (n+1)}{2}$ and the eigenvector is $\mathbf{v}_1 = \mathbf{1}$.
    \item $\lambda_k = d$ and the eigenvectors are $\mathbf{v}_k = {\mathbf{e}}_1 - {{\mathbf{e}}_k}/k$ for $k \in \left\{2, 3, \ldots n\right\}$
\end{itemize}


\section{exercise 19}
\section{exercise 20}
\section{exercise 21}

\section{exercise 22}
\section{exercise 23 Daniel}
The covariance matrix $\mathbf{\Sigma}$ is symmetric and positive definite,
so we can find an Cholesky decomposition,
using which we can define the random vector $\mathbf{X}$ that has the right properties.

\[{\mathbf{\Sigma }} = {\mathbf{L}}{{\mathbf{L}}^T}\qquad {\mathbf{LZ}} = {\mathbf{X}} \Rightarrow Var\left( {\mathbf{X}} \right) = {\mathbf{L}}{{\mathbf{L}}^T} = {\mathbf{\Sigma }}\]

\[{\mathbf{L}} = \left( {\begin{array}{*{20}{c}}
            a & 0 \\
            d & c
        \end{array}} \right) \Rightarrow \begin{array}{*{20}{c}}
        {{a^2} = 9} \\
        {ab =  - 4} \\
        {{b^2} + {c^2} = 16}
    \end{array} \Rightarrow \begin{array}{*{20}{c}}
        {a = 3}      \\
        {b =  - 4/3} \\
        {c = 8\sqrt 2 /3}
    \end{array} \Rightarrow \begin{array}{*{20}{c}}
        {{X_1} = 3{Z_1}} \\
        {{X_2} =  - 4{Z_1}/3 + 8\sqrt 2 {Z_2}/3}
    \end{array}
\]

\section{exercise 24}
Use notation
\[\left( {\begin{array}{*{20}{c}}
            {{X_1}} \\
            {{X_2}} \\
            {{X_3}}
        \end{array}} \right) = {\mathbf{X}}\sim\mathcal{N}\left( {{\mathbf{\mu }},{\mathbf{\Sigma }}} \right)\]

Let ${\mathbf{\hat X}} = {\mathbf{X}} - {\mathbf{\mu }}$,
then $Var\left( {{\mathbf{\hat X}}} \right) = Var\left( {\mathbf{X}} \right) = {\mathbf{\Sigma }}$.
\[P\left( {{{\mathbf{a}}^T}{\mathbf{X}} > 0} \right) = P\left( {{{\mathbf{a}}^T}\left( {{\mathbf{\mu }} + {\mathbf{\hat X}}} \right) > 0} \right) = P\left( {\underbrace {{{\mathbf{a}}^T}{\mathbf{\mu }} + {{\mathbf{a}}^T}{\mathbf{\hat X}}}_Y} > 0 \right)\]

What is the distribution of $Y$?
\[E\left( {{{\mathbf{a}}^T}{\mathbf{\mu }} + {{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = E\left( {{{\mathbf{a}}^T}{\mathbf{\mu }}} \right) + E\left( {{{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = {{\mathbf{a}}^T}{\mathbf{\mu }} = -4\]
\[Var\left( {{{\mathbf{a}}^T}{\mathbf{\mu }} + {{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = Var\left( {{{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = {{\mathbf{a}}^T}Var\left( {{\mathbf{\hat X}}} \right){\mathbf{a}} = {{\mathbf{a}}^T}{\mathbf{\Sigma a}} = 32\]
We also know that $Y$ is a linear combination of normally distributed variables, so it is also normally distributed:
\[Y\sim \mathcal N({{\mathbf{a}}^T}{\mathbf{\mu }}, {{\mathbf{a}}^T}{\mathbf{\Sigma a}}) = \mathcal N(-4, 32)\]
and
\[Z=\frac{Y - E(Y)}{\sqrt{Var(Y)}} = \frac{Y + 4}{\sqrt{32}} \sim \mathcal N(0, 1)\]
\[P\left( {{{\mathbf{a}}^T}{\mathbf{X}} > 0} \right) = P\left( {Y > 0} \right) = P\left( {\frac{{Y + 4}}{{\sqrt {32} }} > \frac{4}{{\sqrt {32} }}} \right) = P\left( {Z > \frac{4}{{\sqrt {32} }}} \right) \approx 0.24\]
\end{document}
