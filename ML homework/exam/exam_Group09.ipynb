{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baruch ML exam group 09, Annie Yi and Daniel Tuzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and preview the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "california_housing = fetch_california_housing(as_frame=True)\n",
    "print(california_housing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing.target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(california_housing.target, bins=30, alpha=0.5, label='y_train')\n",
    "plt.xlabel('House Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of house values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that the distribution has a cut off at 5(MM?), and house values above 5MM may be squeezed to the value 5MM. We don't the details of the data collection, and leave it as it is, believing it is not DQ issue.\n",
    "\n",
    "Show the correlation between the features and the house prices as target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show correlation between the columns of california_housing.data\n",
    "corr_matrix = california_housing.data.join(california_housing.target).corr()\n",
    "display(corr_matrix)\n",
    "# color the correlation matrix, define plt\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map shows that MedHouseVal is positively correlated with MedInc, HouseAge, and AveRooms, and negative with others.\n",
    "\n",
    "- It is interesting that AveBedrooms is negatively correlated with MedHouseVal, which means that the more bedrooms, the less the house price. But the anti-correlation is not strong.\n",
    "- The negative correlation betwwen population and house price means that that some of the most expensive houses are in less populated ares. To have a more detailed view on the pair distributions, below a plot of the pariwise correlations.\n",
    "- Geography does play a role in the house prices, but we expact that it should be correlated with regional blocks, and not latitude and longitude.\n",
    "- The more people live in 1 household, the lower the price is, because rich families with 1 kid or no kids can afford a more expensive house. Families with more kids can spend less on housing. The effect that more poeple must live in more bedrooms is not strong enough to compensate for the effect of the number of bedrooms on the price.\n",
    "- There is a strong correlation between the average number of rooms and average number of bedrooms, so will take only one of the for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(california_housing.data.join(california_housing.target), diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "Based on the previous observations, and to be able to explain the model, we will use the following features: MedInc, HouseAge, AveRooms, AveOccup and Population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and split the data\n",
    "We restrict our data to the features we selected, and split the data into training and test sets with a 80/20 split. We also randomize the data in case the data is ordered in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = california_housing.data[['MedInc', 'HouseAge', 'AveRooms', 'AveOccup', \"Population\"]].values\n",
    "# Append a column of ones to the data matrix to account for the bias term\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "y = california_housing.target.values\n",
    "\n",
    "# split the data into trainind and test set\n",
    "split_ratio = 0.8\n",
    "num_samples = X.shape[0]\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(num_samples)  # just reshuffle the indices, now we can split the data in order\n",
    "split_index = int(num_samples * split_ratio)\n",
    "train_indices = indices[:split_index]\n",
    "test_indices = indices[split_index:]\n",
    "X_train, X_test = X[train_indices], X[test_indices]\n",
    "y_train, y_test = y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape  # check the shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can solve the OLS problem as shown in the pdf using numpy (we have done it in HW3 too):\n",
    "$$\\beta ={{\\left( {{X}^{T}}X \\right)}^{-1}}Xy$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well we perform on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the target values for the train set\n",
    "y_train_pred = X_train @ B\n",
    "# plot the predicted values against the actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_train, y_train_pred, alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "# plot the f(x) = x line\n",
    "plt.plot([0, 5], [0, 5], 'r--')\n",
    "plt.legend(['actual vs trained', 'f(x) = x'])\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Train Set: Actual vs Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well we perform on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the target values for the test set\n",
    "y_test_pred = X_test @ B\n",
    "# plot the predicted values against the actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "# plot the f(x) = x line\n",
    "plt.plot([0, 5], [0, 5], 'r--')\n",
    "plt.legend(['actual vs test', 'f(x) = x'])\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Test Set: Actual vs Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the on the test set, the results are not much worse than on the train set, which is a sign of no high variance. To quanitfy it, we calculate the R2 score, the  residual sum of squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_res_train = np.sum((y_train - y_train_pred)**2)\n",
    "ss_tot_train = np.sum((y_train - np.mean(y_train_pred))**2)\n",
    "r_squared_train = 1 - (ss_res_train / ss_tot_train)\n",
    "r_squared_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_res_test = np.sum((y_test - y_test_pred)**2)\n",
    "ss_tot_test = np.sum((y_test - np.mean(y_test_pred))**2)\n",
    "r_squared_test = 1 - (ss_res_test / ss_tot_test)\n",
    "r_squared_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to implement ridge regression, to eliminate irrelevant features and to avoid overfitting. We will add back the features we dropped out, and also, filter out house prices which are at 5MM to see if it can improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out the 5MM house prices and redo the train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
