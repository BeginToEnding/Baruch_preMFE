{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6c28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1b6e7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "get_ipython().run_line_magic('pip', 'install yfinance numpy pandas scikit-learn xgboost matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f4724",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fetch_and_preprocess_data(ticker, start='2015-01-01', end=None):\n",
    "    if end is None:\n",
    "        end = datetime.date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Force only one ticker, no grouping\n",
    "    df = yf.download(\n",
    "        ticker,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        progress=False,\n",
    "        auto_adjust=False,\n",
    "        group_by=\"column\"  # or omit group_by entirely\n",
    "    )\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # If the columns are still a MultiIndex, flatten them:\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]\n",
    "\n",
    "    # Now do your computations\n",
    "    df['Return'] = df['Adj Close'].pct_change()\n",
    "    df['Volatility_5'] = df['Return'].rolling(window=5).std()\n",
    "    df['SMA_5'] = df['Adj Close'].rolling(window=5).mean()\n",
    "    df['SMA_20'] = df['Adj Close'].rolling(window=20).mean()\n",
    "    \n",
    "    rolling_std_20 = df['Adj Close'].rolling(window=20).std()\n",
    "    df['BB_Mid'] = df['SMA_20']\n",
    "    df['BB_Upper'] = df['BB_Mid'] + 2 * rolling_std_20\n",
    "    df['BB_Lower'] = df['BB_Mid'] - 2 * rolling_std_20\n",
    "\n",
    "    # RSI\n",
    "    def compute_RSI(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "        rs = gain / loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "    \n",
    "    df['RSI_14'] = compute_RSI(df['Adj Close'], 14)\n",
    "\n",
    "    df['Volume_5_SMA'] = df['Volume'].rolling(window=5).mean()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a19044",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_datasets(tickers, start='2015-01-01', end=None, tau=0.005):\n",
    "    \"\"\"\n",
    "    Downloads data for each ticker, computes features, applies label columns,\n",
    "    and returns combined DataFrames for daily, weekly, monthly.\n",
    "    \"\"\"\n",
    "    all_daily = []\n",
    "    all_weekly = []\n",
    "    all_monthly = []\n",
    "\n",
    "    # We'll define a set of feature columns we want to include in the final dataset\n",
    "    feature_cols = [\n",
    "        'Return',\n",
    "        'Volatility_5',\n",
    "        'SMA_5',\n",
    "        'SMA_20',\n",
    "        'BB_Upper',\n",
    "        'BB_Mid',\n",
    "        'BB_Lower',\n",
    "        'RSI_14',\n",
    "        'Volume',\n",
    "        'Volume_5_SMA'\n",
    "    ]\n",
    "\n",
    "    for t in tickers:\n",
    "        df_t = fetch_and_preprocess_data(t, start, end)\n",
    "        df_t = prepare_label_columns(df_t, tau)\n",
    "\n",
    "        # For daily\n",
    "        df_daily = df_t.dropna(subset=['Daily_Labels']).copy()\n",
    "        df_daily['Ticker'] = t\n",
    "        daily_subset = df_daily[feature_cols + ['Daily_Labels']]\n",
    "        all_daily.append(daily_subset)\n",
    "\n",
    "        # For weekly\n",
    "        df_weekly = df_t.dropna(subset=['Weekly_Labels']).copy()\n",
    "        df_weekly['Ticker'] = t\n",
    "        weekly_subset = df_weekly[feature_cols + ['Weekly_Labels']]\n",
    "        all_weekly.append(weekly_subset)\n",
    "\n",
    "        # For monthly\n",
    "        df_monthly = df_t.dropna(subset=['Monthly_Labels']).copy()\n",
    "        df_monthly['Ticker'] = t\n",
    "        monthly_subset = df_monthly[feature_cols + ['Monthly_Labels']]\n",
    "        all_monthly.append(monthly_subset)\n",
    "\n",
    "    # Combine all tickers into 3 big dataframes\n",
    "    df_daily_all = pd.concat(all_daily, axis=0)\n",
    "    df_weekly_all = pd.concat(all_weekly, axis=0)\n",
    "    df_monthly_all = pd.concat(all_monthly, axis=0)\n",
    "\n",
    "    return df_daily_all, df_weekly_all, df_monthly_all, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb68b7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_and_evaluate_classifiers(df, feature_cols, label_col, random_state=42):\n",
    "    X = df[feature_cols].values\n",
    "    y_original = df[label_col].values\n",
    "\n",
    "    # Remap labels:\n",
    "    #   -1 -> 0  (negative)\n",
    "    #    0 -> 1  (neutral)\n",
    "    #   +1 -> 2  (positive)\n",
    "    label_map = {-1: 0, 0: 1, 1: 2}\n",
    "    y = np.array([label_map[val] for val in y_original])\n",
    "\n",
    "    # Now y is in [0, 1, 2], which XGBoost expects for multiclass\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, shuffle=True, random_state=random_state\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    models = {\n",
    "        'LogisticRegression': LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            multi_class='multinomial',\n",
    "            random_state=random_state\n",
    "        ),\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            random_state=random_state\n",
    "        ),\n",
    "        'XGBClassifier': XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='mlogloss',\n",
    "            random_state=random_state\n",
    "        )\n",
    "    }\n",
    "\n",
    "    trained_models = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if name == 'LogisticRegression':\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "        # Print metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # For a nice classification_report with custom labels:\n",
    "        target_names = ['Negative', 'Neutral', 'Positive']\n",
    "        cr = classification_report(y_test, y_pred, digits=3, target_names=target_names)\n",
    "\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "        print(\"Classification Report:\\n\", cr)\n",
    "\n",
    "        trained_models[name] = (model, scaler)\n",
    "\n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f852e7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tickers = [\n",
    "    'AAPL',  # Tech\n",
    "    'MSFT',  # Tech\n",
    "    'JNJ',   # Healthcare\n",
    "    'PFE',   # Pharma\n",
    "    'XOM',   # Energy\n",
    "    'CVX',   # Energy\n",
    "    'JPM',   # Financial\n",
    "    'BAC',   # Financial\n",
    "    'WMT',   # Consumer Defensive\n",
    "    'TGT',   # Consumer Defensive\n",
    "    'HD',    # Consumer Cyclical\n",
    "    'MCD',   # Restaurant/Retail\n",
    "    'TSLA',  # Automotive/Tech\n",
    "    'F',     # Automotive\n",
    "    'BA',    # Aerospace\n",
    "    'CAT',   # Industrials\n",
    "    'IBM',   # Tech\n",
    "    'AMZN',  # Tech/Retail\n",
    "    'GOOGL', # Tech\n",
    "    'META'   # Tech\n",
    "]\n",
    "\n",
    "print(\"Building datasets (daily, weekly, monthly)...\")\n",
    "df_daily_all, df_weekly_all, df_monthly_all, feature_cols = build_datasets(\n",
    "    tickers,\n",
    "    start='2015-01-01',\n",
    "    end=None,      # up to today's date\n",
    "    tau=0.005      # Â±0.5% threshold for neutral\n",
    ")\n",
    "\n",
    "print(\"Datasets built:\")\n",
    "print(f\"  Daily:   {df_daily_all.shape[0]} rows\")\n",
    "print(f\"  Weekly:  {df_weekly_all.shape[0]} rows\")\n",
    "print(f\"  Monthly: {df_monthly_all.shape[0]} rows\")\n",
    "\n",
    "# 6.2: Train & Evaluate for Daily\n",
    "print(\"\\n===== DAILY PREDICTION =====\")\n",
    "daily_models = train_and_evaluate_classifiers(df_daily_all, feature_cols, 'Daily_Labels')\n",
    "\n",
    "# 6.3: Train & Evaluate for Weekly\n",
    "print(\"\\n===== WEEKLY PREDICTION =====\")\n",
    "weekly_models = train_and_evaluate_classifiers(df_weekly_all, feature_cols, 'Weekly_Labels')\n",
    "\n",
    "# 6.4: Train & Evaluate for Monthly\n",
    "print(\"\\n===== MONTHLY PREDICTION =====\")\n",
    "monthly_models = train_and_evaluate_classifiers(df_monthly_all, feature_cols, 'Monthly_Labels')\n",
    "\n",
    "print(\"\\nAll done! Check the accuracy, confusion matrices, and classification reports above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea615c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
